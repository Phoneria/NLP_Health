{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl.utils.exceptions import IllegalCharacterError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and extract text from a webpage\n",
    "def fetch_webpage_text(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # Some websites block requests without user-agent\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise error for bad responses (4xx, 5xx)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract text from the page\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        return text\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None  # Return None if there was an error\n",
    "    \n",
    "# Function to clean illegal characters\n",
    "def clean_illegal_characters(value):\n",
    "    if isinstance(value, str):\n",
    "        return \"\".join(c if ord(c) >= 32 else \"***\" for c in value)  # Replace control characters with ***\n",
    "    return value  # Return original value if not a string\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Specify the directory containing Excel files\n",
    "directory = r\"C:\\Users\\AZMI\\Desktop\\Izzet Ahmet\\Kodlar\\Medical\\Data\\Raw\"\n",
    "\n",
    "\n",
    "# Get all Excel files in the directory\n",
    "xlsx_files = glob.glob(os.path.join(directory, \"*.xlsx\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xlsx_file in xlsx_files:\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(xlsx_file)\n",
    "\n",
    "        # Check if 'WEB_ADDRESS' column exists\n",
    "        if \"WEB_ADDRESS\" not in df.columns:\n",
    "            print(f\"Skipping {xlsx_file} (No 'WEB_ADDRESS' column found)\")\n",
    "            continue\n",
    "\n",
    "        # Fetch webpage text\n",
    "        df[\"PAGE_TEXT\"] = df[\"WEB_ADDRESS\"].progress_apply(fetch_webpage_text)\n",
    "\n",
    "        # Apply the extraction function\n",
    "        #df_extracted = df[\"PAGE_TEXT\"].apply(lambda text: extract_values(text, keys)).apply(pd.Series)\n",
    "\n",
    "        # Merge extracted data with original DataFrame\n",
    "        #df = pd.concat([df, df_extracted], axis=1)\n",
    "\n",
    "        # Drop PAGE_TEXT column\n",
    "        #df = df.drop(columns=[\"PAGE_TEXT\"], axis=1)\n",
    "\n",
    "        # Define new file name\n",
    "        new_file_name = os.path.splitext(xlsx_file)[0] + \"_processed.xlsx\"\n",
    "\n",
    "\n",
    "        # Apply function to all text data in DataFrame\n",
    "        df_cleaned = df.applymap(clean_illegal_characters)\n",
    "        # Save to Excel\n",
    "        df_cleaned.to_excel(new_file_name, index=False)\n",
    "        print(f\"Saved processed file: {new_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the keys to extract\n",
    "keys = [\n",
    "    \"Date Initiated by Firm\",\n",
    "    \"Date Posted\",\n",
    "    \"Recall Status\",\n",
    "    \"Recall Number\",\n",
    "    \"Recall Event ID\",\n",
    "    \"510(K) Number\",\n",
    "    \"Product Classification\",\n",
    "    \"Product\",\n",
    "    \"Code Information\",\n",
    "    \"Recalling Firm/Manufacturer\",\n",
    "    \"Manufacturer Reason for Recall\",\n",
    "    \"FDA Determined Cause\",\n",
    "    \"Action\",\n",
    "    \"Quantity in Commerce\",\n",
    "    \"Distribution\",\n",
    "    \"Total Product Life Cycle\"\n",
    "]\n",
    "\n",
    "# Function to extract values from PAGE_TEXT\n",
    "def extract_values(text, keys):\n",
    "    extracted_data = {}\n",
    "    for i, key in enumerate(keys):\n",
    "        # Create regex pattern to match key-value pairs\n",
    "        pattern = rf\"{key}\\s*(.*?)(?=\\n{keys[i+1]}|\\Z)\" if i + 1 < len(keys) else rf\"{key}\\s*(.*)\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        extracted_data[key] = match.group(1).strip() if match else None\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
